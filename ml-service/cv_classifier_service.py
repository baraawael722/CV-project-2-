"""
CV Classification Service
ÙŠØ³ØªØ®Ø¯Ù… Ù…ÙˆØ¯ÙŠÙ„ cv_classifier_merged.keras Ù…Ø¹ Groq API Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø³ÙŠØ± Ø§Ù„Ø°Ø§ØªÙŠØ©
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import tensorflow as tf
import numpy as np
import os
import re
import sys
from typing import Optional
import json

# Ensure UTF-8 stdout to avoid Windows encoding errors with logs
sys.stdout.reconfigure(encoding="utf-8")

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Groq API
try:
    from groq import Groq
    GROQ_AVAILABLE = True
except ImportError:
    GROQ_AVAILABLE = False
    print("âš ï¸ Groq library not available. Will use only Keras model.")

app = FastAPI(title="CV Classification Service")

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ¯ÙŠÙ„ Keras
MODEL_PATH = "cv_classifier_merged.keras"
model = None
groq_client = None
JOB_CATEGORIES = []  # Ø³ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ù…Ù† Ù…Ù„Ù JSON


class CVClassificationRequest(BaseModel):
    cv_text: str
    use_groq_analysis: bool = True


class CVClassificationResponse(BaseModel):
    success: bool
    job_title: str
    confidence: float
    decision_method: Optional[str] = None
    ai_analysis: Optional[dict] = None
    keras_prediction: Optional[dict] = None
    error: Optional[str] = None


def load_model():
    """ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ¯ÙŠÙ„ Keras ÙˆØ§Ù„ÙØ¦Ø§Øª"""
    global model, JOB_CATEGORIES
    try:
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙØ¦Ø§Øª Ù…Ù† Ù…Ù„Ù JSON
        classes_path = "job_classes.json"
        if os.path.exists(classes_path):
            with open(classes_path, 'r', encoding='utf-8') as f:
                JOB_CATEGORIES = json.load(f)
            print(f"âœ… Loaded {len(JOB_CATEGORIES)} job categories")
        else:
            print(f"âš ï¸ Classes file not found at {classes_path}")
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‚Ø§Ø¦Ù…Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
            JOB_CATEGORIES = ["Software Engineer", "Data Scientist", "Web Developer"]
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
        if os.path.exists(MODEL_PATH):
            model = tf.keras.models.load_model(MODEL_PATH)
            print(f"âœ… Keras model loaded successfully from {MODEL_PATH}")
            print(f"   Input shape: {model.input_shape}")
            print(f"   Output shape: {model.output_shape}")
        else:
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ÙÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
            parent_model_path = os.path.join("..", MODEL_PATH)
            if os.path.exists(parent_model_path):
                model = tf.keras.models.load_model(parent_model_path)
                print(f"âœ… Keras model loaded from parent directory: {parent_model_path}")
                print(f"   Input shape: {model.input_shape}")
                print(f"   Output shape: {model.output_shape}")
            else:
                print(f"âš ï¸ Model file not found at {MODEL_PATH}")
                print(f"âš ï¸ Also checked: {parent_model_path}")
                model = None
    except Exception as e:
        print(f"âŒ Error loading Keras model: {e}")
        model = None


def initialize_groq():
    """ØªÙ‡ÙŠØ¦Ø© Groq API"""
    global groq_client
    if not GROQ_AVAILABLE:
        return
    
    api_key = os.getenv("GROQ_API_KEY")
    if api_key:
        try:
            groq_client = Groq(api_key=api_key)
            print("âœ… Groq client initialized successfully")
        except Exception as e:
            print(f"âŒ Error initializing Groq: {e}")
            groq_client = None
    else:
        print("âš ï¸ GROQ_API_KEY not found in environment variables")


@app.on_event("startup")
async def startup_event():
    """ØªØ´ØºÙŠÙ„ Ø¹Ù†Ø¯ Ø¨Ø¯Ø¡ Ø§Ù„Ø³ÙŠØ±ÙØ±"""
    print("ğŸš€ Starting CV Classification Service...")
    load_model()
    initialize_groq()
    print("âœ… Service ready!")


def extract_text_features(text: str) -> np.ndarray:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ features Ù…Ù† Ø§Ù„Ù†Øµ - Ø¹Ù…Ù„ text padding Ù„ 8000 characters
    Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ÙŠØªÙˆÙ‚Ø¹ CV text Ø¨Ø·ÙˆÙ„ Ù…Ø­Ø¯Ø¯ (8000)
    
    Ø§Ø³ØªØ®Ø¯Ø§Ù… TF-IDF Ø£Ùˆ character-level encoding
    """
    # Ù†Ø¸Ù Ø§Ù„Ù†Øµ ÙˆØ­ÙˆÙ„Ù‡ Ù„Ø£Ø­Ø±Ù ØµØºÙŠØ±Ø©
    text = text.lower().strip()
    
    # Pad Ø£Ùˆ truncate Ø¥Ù„Ù‰ 8000 characters Ø¨Ø§Ù„Ø¶Ø¨Ø·
    if len(text) > 8000:
        text = text[:8000]
    elif len(text) < 8000:
        # Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ù…Ø¬Ø±Ø¯ spacesØŒ Ø§Ø³ØªØ®Ø¯Ù… padding Ø°ÙƒÙŠ
        text = text + '\n' * (8000 - len(text))
    
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ character-level features
    features = []
    for char in text:
        # ØªØ­ÙˆÙŠÙ„ ÙƒÙ„ Ø­Ø±Ù Ø¥Ù„Ù‰ Ù‚ÙŠÙ…Ø© ÙˆØªØ·Ø¨ÙŠØ¹Ù‡Ø§
        # Ø§Ø³ØªØ®Ø¯Ù… ord() Ø¨Ø´ÙƒÙ„ Ø£ÙØ¶Ù„
        if char == '\n':
            features.append(0.0)  # newline
        elif char == ' ':
            features.append(0.1)  # space
        else:
            # normalize ASCII value between 0.1 and 1.0
            ascii_val = ord(char)
            if ascii_val < 32:  # control characters
                features.append(0.05)
            else:
                # Map printable characters (32-126) to 0.2-1.0
                features.append(min(max((ascii_val - 32) / (126 - 32) * 0.8 + 0.2, 0.2), 1.0))
    
    # ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø­Ø¬Ù… Ø¨Ø§Ù„Ø¶Ø¨Ø· 8000
    features_array = np.array(features, dtype=np.float32).reshape(1, 8000)
    
    return features_array


def classify_with_keras_model(cv_text: str) -> dict:
    """ØªØµÙ†ÙŠÙ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙˆØ¯ÙŠÙ„ Keras"""
    if model is None:
        return {"error": "Model not loaded"}
    
    try:
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ features
        features = extract_text_features(cv_text)
        print(f"ğŸ“Š Features shape: {features.shape}")
        
        # Ø§Ù„ØªÙ†Ø¨Ø¤
        predictions = model.predict(features, verbose=0)
        print(f"ğŸ“Š Predictions shape: {predictions.shape}")
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø¹Ù„Ù‰ 3 ØªÙ†Ø¨Ø¤Ø§Øª
        top_3_indices = np.argsort(predictions[0])[-3:][::-1]
        top_3_scores = predictions[0][top_3_indices]
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ ØªÙ†Ø¨Ø¤
        predicted_index = int(top_3_indices[0])
        confidence = float(top_3_scores[0])
        
        if predicted_index < len(JOB_CATEGORIES):
            predicted_job = JOB_CATEGORIES[predicted_index]
        else:
            predicted_job = f"Class_{predicted_index}"
        
        # ØªØ¬Ù‡ÙŠØ² top 3
        top_predictions = []
        for idx, score in zip(top_3_indices, top_3_scores):
            job_name = JOB_CATEGORIES[int(idx)] if int(idx) < len(JOB_CATEGORIES) else f"Class_{idx}"
            top_predictions.append({
                "job_title": job_name,
                "confidence": float(score)
            })
        
        return {
            "predicted_job": predicted_job,
            "confidence": confidence,
            "method": "keras_model",
            "top_3_predictions": top_predictions,
            "total_classes": len(JOB_CATEGORIES)
        }
        
    except Exception as e:
        print(f"âŒ Error in Keras prediction: {e}")
        import traceback
        traceback.print_exc()
        return {"error": str(e)}


def detect_domain_role(text_lower: str) -> Optional[str]:
    """Ø§ÙƒØªØ´Ø§Ù Ø¯ÙˆØ± Ø¹Ø§Ù… Ù…Ù† ÙƒÙ„Ù…Ø§Øª Ù†Ø·Ø§Ù‚ ØºÙŠØ± ØªÙ‚Ù†ÙŠ Ù…Ø«Ù„ Ø§Ù„Ø±Ø¹Ø§ÙŠØ© Ø§Ù„ØµØ­ÙŠØ©"""
    healthcare_terms = [
        'hospital', 'clinic', 'patient', 'healthcare', 'medical', 'doctor', 'nurse',
        'pharmacy', 'pharmacist', 'therapist', 'surgery', 'laboratory', 'radiology'
    ]
    if any(term in text_lower for term in healthcare_terms):
        return "Healthcare Professional"
    return None


def extract_analysis_from_text(cv_text: str) -> dict:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ù…Ù† Ø§Ù„Ù†Øµ Ù…Ø¨Ø§Ø´Ø±Ø© (Ø¨Ø¯ÙˆÙ† API)"""
    text_lower = cv_text.lower()
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ù‡Ø§Ø±Ø§Øª
    all_skills = [
        'python', 'javascript', 'typescript', 'java', 'c++', 'c#', 'php', 'ruby', 'go', 'rust',
        'react', 'vue', 'angular', 'nodejs', 'express', 'django', 'flask', 'spring',
        'mongodb', 'postgresql', 'mysql', 'redis', 'docker', 'kubernetes',
        'aws', 'azure', 'gcp', 'git', 'linux', 'html', 'css', 'sql',
        'machine learning', 'tensorflow', 'pytorch', 'pandas', 'numpy', 'rest api'
    ]
    
    found_skills = [skill for skill in all_skills if skill in text_lower]
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©
    experience_years = 0
    import re
    years_match = re.search(r'(\d+)\s*(?:years?|yrs?|Ø³Ù†Ø©|Ø³Ù†ÙˆØ§Øª)', text_lower)
    if years_match:
        experience_years = int(years_match.group(1))
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ©
    languages = []
    lang_keywords = {
        'Python': 'python',
        'JavaScript': 'javascript',
        'Java': 'java',
        'C++': 'c++',
        'C#': 'c#',
        'PHP': 'php',
        'Ruby': 'ruby',
        'Go': 'go',
        'TypeScript': 'typescript'
    }
    
    for lang_name, keyword in lang_keywords.items():
        if keyword in text_lower:
            languages.append(lang_name)

    # ØªØ­Ø¯ÙŠØ¯ Ø¯ÙˆØ± Ø¹Ø§Ù… (Ù…Ø«Ù„ Ø§Ù„Ø±Ø¹Ø§ÙŠØ© Ø§Ù„ØµØ­ÙŠØ©) Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…Ù‡Ø§Ø±Ø§Øª ØªÙ‚Ù†ÙŠØ© Ù…ÙˆØ¬ÙˆØ¯Ø©
    domain_role = detect_domain_role(text_lower)
    primary_role = domain_role or "Software Developer"
    
    return {
        "primary_role": primary_role,
        "skills": found_skills[:15],  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ 15 Ù…Ù‡Ø§Ø±Ø©
        "experience_years": experience_years,
        "languages": languages,
        "projects": [],
        "recommended_categories": []
    }


def analyze_cv_with_groq(cv_text: str) -> dict:
    """ØªØ­Ù„ÙŠÙ„ CV Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Groq API"""
    if not groq_client:
        return {"error": "Groq client not available"}
    
    prompt = f"""
Analyze this CV and provide detailed information about the candidate's profile:

CV Text:
{cv_text}

Please provide:
1. Primary job role/title that best fits this candidate
2. Key technical skills mentioned
3. Years of experience (estimate if not explicitly stated)
4. Main programming languages
5. Notable projects or achievements
6. Recommended job categories (from: Frontend Developer, Backend Developer, Full Stack Developer, Mobile Developer, DevOps Engineer, Data Scientist, Machine Learning Engineer, UI/UX Designer, Software Engineer, Quality Assurance Engineer)

Format your response as JSON with these fields:
{{
    "primary_role": "...",
    "skills": ["skill1", "skill2", ...],
    "experience_years": number,
    "languages": ["lang1", "lang2", ...],
    "projects": ["project1", "project2", ...],
    "recommended_categories": ["category1", "category2", ...]
}}
"""
    
    try:
        chat_completion = groq_client.chat.completions.create(
            messages=[
                {
                    "role": "user",
                    "content": prompt,
                }
            ],
            model="llama3-8b-8192",  # Ø£Ùˆ Ø£ÙŠ Ù…ÙˆØ¯ÙŠÙ„ Ù…ØªØ§Ø­
            temperature=0.3,
            max_tokens=1024,
        )
        
        response_text = chat_completion.choices[0].message.content
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ JSON Ù…Ù† Ø§Ù„Ø±Ø¯
        json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
        if json_match:
            analysis = json.loads(json_match.group())
            return analysis
        else:
            return {"raw_response": response_text}
            
    except Exception as e:
        return {"error": str(e)}


def classify_with_keywords(cv_text: str) -> dict:
    """ØªØµÙ†ÙŠÙ Ø¨Ø³ÙŠØ· Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… keyword matching"""
    text_lower = cv_text.lower()
    
    # ØªØ¹Ø±ÙŠÙ keywords Ù„ÙƒÙ„ ÙØ¦Ø©
    job_keywords = {
        "Frontend Developer": ['react', 'vue', 'angular', 'javascript', 'html', 'css', 'frontend', 'ui', 'typescript', 'next.js'],
        "Backend Developer": ['node', 'python', 'java', 'django', 'flask', 'spring', 'backend', 'api', 'express', 'fastapi'],
        "Full Stack Developer": ['full stack', 'fullstack', 'mern', 'mean', 'full-stack', 'lamp'],
        "Mobile Developer": ['android', 'ios', 'react native', 'flutter', 'swift', 'kotlin', 'mobile', 'app'],
        "DevOps Engineer": ['docker', 'kubernetes', 'aws', 'azure', 'devops', 'ci/cd', 'jenkins', 'terraform'],
        "Data Scientist": ['data science', 'machine learning', 'pandas', 'numpy', 'python', 'tensorflow', 'pytorch'],
        "Machine Learning Engineer": ['machine learning', 'deep learning', 'ai', 'neural', 'tensorflow', 'pytorch', 'keras'],
    }
    
    # Ø§Ø­Ø³Ø¨ score Ù„ÙƒÙ„ ÙØ¦Ø©
    scores = {}
    for job_title, keywords in job_keywords.items():
        score = sum(1 for keyword in keywords if keyword in text_lower)
        scores[job_title] = score
    
    # Ø§Ø­ØµÙ„ Ø¹Ù„Ù‰ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø£Ø¹Ù„Ù‰
    best_job = max(scores, key=scores.get)
    best_score = scores[best_job]
    
    # Ø­ÙˆÙ„ score Ø¥Ù„Ù‰ confidence (normalized)
    max_possible_score = max(len(kw) for kw in job_keywords.values())
    confidence = min(best_score / max_possible_score * 100, 100) / 100
    confidence = max(confidence, 0.5)  # Ø­Ø¯ Ø£Ø¯Ù†Ù‰ 50% Ø¥Ø°Ø§ ÙˆØ¬Ø¯Ù†Ø§ Ø£ÙŠ keywords
    
    if best_score == 0:
        confidence = 0.0
    
    return {
        "predicted_job": best_job,
        "confidence": confidence,
        "method": "keyword_matching",
        "scores": scores
    }


@app.post("/classify", response_model=CVClassificationResponse)
async def classify_cv(request: CVClassificationRequest):
    """
    ØªØµÙ†ÙŠÙ CV Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Hybrid Approach: Keras Model + Keyword Matching + AI Analysis
    """
    try:
        cv_text = request.cv_text.strip()
        
        if not cv_text:
            print("âŒ CV text is empty")
            raise HTTPException(status_code=400, detail="CV text is required")
        
        print(f"\n{'='*60}")
        print(f"ğŸ“„ CV Text Length: {len(cv_text)} characters")
        print(f"ğŸ“š First 200 chars: {cv_text[:200]}")
        print(f"{'='*60}\n")
        
        # 1. Ø§Ø³ØªØ®Ø¯Ø§Ù… Keyword Matching Ø£ÙˆÙ„Ø§Ù‹ (baseline)
        print("ğŸ” Step 1: Keyword Matching...")
        keyword_result = classify_with_keywords(cv_text)
        keyword_job = keyword_result.get("predicted_job", "Unknown")
        keyword_confidence = keyword_result.get("confidence", 0.0)
        keyword_scores = keyword_result.get("scores", {})
        max_keyword_score = max(keyword_scores.values()) if keyword_scores else 0
        print(f"   ğŸ“Š Keyword: {keyword_job} ({keyword_confidence*100:.1f}%) | score={max_keyword_score}")
        
        # 2. Ø§Ø³ØªØ®Ø¯Ø§Ù… Keras Model (Ø¥Ø°Ø§ Ù…ØªØ§Ø­)
        keras_result = None
        keras_job = None
        keras_confidence = 0.0
        
        if model is not None:
            print("ğŸ§  Step 2: Keras Model Classification...")
            keras_result = classify_with_keras_model(cv_text)
            
            if "error" not in keras_result:
                keras_job = keras_result.get("predicted_job", "Unknown")
                keras_confidence = keras_result.get("confidence", 0.0)
                print(f"   ğŸ“Š Keras: {keras_job} ({keras_confidence*100:.1f}%)")
            else:
                print(f"   âŒ Keras error: {keras_result['error']}")
        
        # 3. Ø¯Ù…Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨Ø°ÙƒØ§Ø¡ (Ensemble)
        print("\nğŸ”„ Step 3: Ensemble Decision...")
        final_job_title = "Unknown"
        final_confidence = 0.0
        decision_method = "keyword_primary"
        
        # Ø§Ø³ØªØ®Ø¯Ù… Keywords ÙƒØ£Ø³Ø§Ø³ (Ù„Ø£Ù† Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ØºÙŠØ± Ù…ÙˆØ«ÙˆÙ‚)
        final_job_title = keyword_job
        final_confidence = keyword_confidence
        
        # Ø­Ø§Ù„Ø© 1: Keyword matching Ù‚ÙˆÙŠ (>= 3 matches) - Ø§Ø³ØªØ®Ø¯Ù…Ù‡ Ù…Ø¨Ø§Ø´Ø±Ø©
        if max_keyword_score >= 3:
            decision_method = "keyword_strong"
            # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø«Ù‚Ø© Ù‚Ù„ÙŠÙ„Ø§Ù‹ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Keywords Ù‚ÙˆÙŠØ©
            final_confidence = min(keyword_confidence * 1.1, 0.95)
            print(f"   âœ… Strong keyword match ({max_keyword_score} matches)")
        
        # Ø­Ø§Ù„Ø© 2: Keyword matching Ù…ØªÙˆØ³Ø· (1-2 matches)
        elif max_keyword_score >= 1:
            decision_method = "keyword_moderate"
            print(f"   âœ“ Moderate keyword match ({max_keyword_score} matches)")
            
            # Ø¥Ø°Ø§ ÙƒØ§Ù† Keras ÙŠØªÙÙ‚ Ù…Ø¹ KeywordsØŒ Ø²Ø¯ Ø§Ù„Ø«Ù‚Ø©
            if keras_job and keras_job == keyword_job:
                final_confidence = min((keyword_confidence + keras_confidence) / 2.0 * 1.15, 0.90)
                decision_method = "keyword_keras_agreement"
                print(f"   âœ… Keras agrees with keywords!")
        
        # Ø­Ø§Ù„Ø© 3: Ù„Ø§ ØªÙˆØ¬Ø¯ keywords ÙˆØ§Ø¶Ø­Ø© (0 matches)
        else:
            print(f"   âš ï¸ No keyword matches found")
            decision_method = "text_analysis"
            
            # Ø§Ø³ØªØ®Ø¯Ù… Text Analysis
            ai_analysis_temp = extract_analysis_from_text(cv_text)
            if ai_analysis_temp and "primary_role" in ai_analysis_temp:
                final_job_title = ai_analysis_temp["primary_role"]
                final_confidence = 0.65
                decision_method = "text_analysis_fallback"
                print(f"   â†’ Using text analysis: {final_job_title}")
            else:
                # Ø¢Ø®Ø± Ù…Ø­Ø§ÙˆÙ„Ø©: Ø§Ø³ØªØ®Ø¯Ù… Keras
                if keras_job:
                    final_job_title = keras_job
                    final_confidence = min(keras_confidence * 0.7, 0.70)  # Ø®ÙØ¶ Ø§Ù„Ø«Ù‚Ø©
                    decision_method = "keras_last_resort"
                    print(f"   â†’ Using Keras as last resort")
                else:
                    final_job_title = "Software Engineer"  # default
                    final_confidence = 0.50
                    decision_method = "default"
                    print(f"   â†’ Using default")
        
        # 4. Ø§Ø®ØªÙŠØ§Ø±ÙŠ: AI Analysis Ù„Ù„ØªØ­Ø³ÙŠÙ†
        ai_analysis = None
        if request.use_groq_analysis or final_confidence < 0.50:
            print("\nğŸ¤– Step 4: AI Analysis...")
            if groq_client:
                ai_analysis = analyze_cv_with_groq(cv_text)
            else:
                ai_analysis = extract_analysis_from_text(cv_text)
            
            if ai_analysis and "primary_role" in ai_analysis:
                ai_role = ai_analysis.get("primary_role")
                print(f"   ğŸ“Š AI: {ai_role}")
                
                # Ø§Ø³ØªØ®Ø¯Ù… AI ÙÙ‚Ø· Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø«Ù‚Ø© Ù…Ù†Ø®ÙØ¶Ø© Ø¬Ø¯Ø§Ù‹
                if final_confidence < 0.50:
                    final_job_title = ai_role
                    final_confidence = 0.70
                    decision_method = "ai_low_confidence"
                    print(f"   âœ… Using AI (low confidence)")
        
        print(f"\n{'='*60}")
        print(f"âœ… FINAL: {final_job_title} ({final_confidence*100:.1f}%) [{decision_method}]")
        print(f"{'='*60}\n")
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
        response_data = {
            "job_title": final_job_title,
            "confidence": final_confidence,
            "decision_method": decision_method,
            "ai_analysis": ai_analysis,
            "keras_prediction": keras_result if keras_result else keyword_result
        }
        
        return CVClassificationResponse(
            success=True,
            **response_data
        )
        
    except Exception as e:
        print(f"âŒ Error in classify_cv: {e}")
        import traceback
        traceback.print_exc()
        return CVClassificationResponse(
            success=False,
            job_title="Error",
            confidence=0.0,
            error=str(e)
        )


@app.get("/")
async def root():
    """ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    return {
        "service": "CV Classification Service",
        "status": "running",
        "keras_model": "loaded" if model else "not loaded",
        "groq_api": "available" if groq_client else "not available",
        "endpoints": {
            "classify": "/classify (POST)",
            "health": "/health (GET)"
        }
    }


@app.get("/health")
async def health():
    """ÙØ­Øµ Ø­Ø§Ù„Ø© Ø§Ù„Ø³ÙŠØ±ÙØ±"""
    return {
        "status": "healthy",
        "keras_model": model is not None,
        "groq_api": groq_client is not None
    }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=5002)
