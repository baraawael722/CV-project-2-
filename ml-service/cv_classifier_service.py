"""
CV Classification Service
ÙŠØ³ØªØ®Ø¯Ù… Ù…ÙˆØ¯ÙŠÙ„ cv_classifier_merged.keras Ù…Ø¹ Groq API Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø³ÙŠØ± Ø§Ù„Ø°Ø§ØªÙŠØ©
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import tensorflow as tf
import numpy as np
import os
import re
import sys
from typing import Optional
import json

# Ensure UTF-8 stdout to avoid Windows encoding errors with logs
sys.stdout.reconfigure(encoding="utf-8")

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Groq API
try:
    from groq import Groq
    GROQ_AVAILABLE = True
except ImportError:
    GROQ_AVAILABLE = False
    print("âš ï¸ Groq library not available. Will use only Keras model.")

app = FastAPI(title="CV Classification Service")

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ¯ÙŠÙ„ Keras
MODEL_PATH = "cv_classifier_merged.keras"
model = None
groq_client = None

# Job titles/categories Ù„Ù„ØªØµÙ†ÙŠÙ
JOB_CATEGORIES = [
    "Frontend Developer",
    "Backend Developer",
    "Full Stack Developer",
    "Mobile Developer",
    "DevOps Engineer",
    "Data Scientist",
    "Machine Learning Engineer",
    "UI/UX Designer",
    "Software Engineer",
    "Quality Assurance Engineer",
    "Database Administrator",
    "Security Engineer",
    "Cloud Engineer",
    "Product Manager",
    "Business Analyst"
]


class CVClassificationRequest(BaseModel):
    cv_text: str
    use_groq_analysis: bool = True


class CVClassificationResponse(BaseModel):
    success: bool
    job_title: str
    confidence: float
    decision_method: Optional[str] = None
    ai_analysis: Optional[dict] = None
    keras_prediction: Optional[dict] = None
    error: Optional[str] = None


def load_model():
    """ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ¯ÙŠÙ„ Keras"""
    global model
    try:
        if os.path.exists(MODEL_PATH):
            model = tf.keras.models.load_model(MODEL_PATH)
            print(f"âœ… Keras model loaded successfully from {MODEL_PATH}")
        else:
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ÙÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
            parent_model_path = os.path.join("..", MODEL_PATH)
            if os.path.exists(parent_model_path):
                model = tf.keras.models.load_model(parent_model_path)
                print(f"âœ… Keras model loaded from parent directory: {parent_model_path}")
            else:
                print(f"âš ï¸ Model file not found at {MODEL_PATH}")
                print(f"âš ï¸ Also checked: {parent_model_path}")
                model = None
    except Exception as e:
        print(f"âŒ Error loading Keras model: {e}")
        model = None


def initialize_groq():
    """ØªÙ‡ÙŠØ¦Ø© Groq API"""
    global groq_client
    if not GROQ_AVAILABLE:
        return
    
    api_key = os.getenv("GROQ_API_KEY")
    if api_key:
        try:
            groq_client = Groq(api_key=api_key)
            print("âœ… Groq client initialized successfully")
        except Exception as e:
            print(f"âŒ Error initializing Groq: {e}")
            groq_client = None
    else:
        print("âš ï¸ GROQ_API_KEY not found in environment variables")


@app.on_event("startup")
async def startup_event():
    """ØªØ´ØºÙŠÙ„ Ø¹Ù†Ø¯ Ø¨Ø¯Ø¡ Ø§Ù„Ø³ÙŠØ±ÙØ±"""
    print("ğŸš€ Starting CV Classification Service...")
    load_model()
    initialize_groq()
    print("âœ… Service ready!")


def extract_text_features(text: str) -> np.ndarray:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ features Ù…Ù† Ø§Ù„Ù†Øµ - Ø¹Ù…Ù„ text padding Ù„ 8000 characters
    Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ÙŠØªÙˆÙ‚Ø¹ CV text Ø¨Ø·ÙˆÙ„ Ù…Ø­Ø¯Ø¯
    """
    text = text.lower()[:8000]  # Ø®Ø° Ø£ÙˆÙ„ 8000 Ø­Ø±Ù
    
    # Pad Ø£Ùˆ truncate Ø¥Ù„Ù‰ 8000 characters
    if len(text) < 8000:
        text = text + ' ' * (8000 - len(text))
    
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ASCII values Ø«Ù… ØªØ·Ø¨ÙŠØ¹Ù‡Ø§
    # Ù‡Ø°Ù‡ Ø·Ø±ÙŠÙ‚Ø© Ø¨Ø³ÙŠØ·Ø© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ 8000 features Ù…Ù† Ø§Ù„Ù†Øµ
    features = []
    for char in text:
        # ØªØ­ÙˆÙŠÙ„ ÙƒÙ„ Ø­Ø±Ù Ø¥Ù„Ù‰ Ù‚ÙŠÙ…Ø© ASCII ÙˆØªØ·Ø¨ÙŠØ¹Ù‡Ø§
        ascii_val = ord(char) / 256.0  # normalize Ø¨ÙŠÙ† 0 Ùˆ 1
        features.append(ascii_val)
    
    features_array = np.array(features, dtype=np.float32).reshape(1, -1)
    return features_array


def detect_domain_role(text_lower: str) -> Optional[str]:
    """Ø§ÙƒØªØ´Ø§Ù Ø¯ÙˆØ± Ø¹Ø§Ù… Ù…Ù† ÙƒÙ„Ù…Ø§Øª Ù†Ø·Ø§Ù‚ ØºÙŠØ± ØªÙ‚Ù†ÙŠ Ù…Ø«Ù„ Ø§Ù„Ø±Ø¹Ø§ÙŠØ© Ø§Ù„ØµØ­ÙŠØ©"""
    healthcare_terms = [
        'hospital', 'clinic', 'patient', 'healthcare', 'medical', 'doctor', 'nurse',
        'pharmacy', 'pharmacist', 'therapist', 'surgery', 'laboratory', 'radiology'
    ]
    if any(term in text_lower for term in healthcare_terms):
        return "Healthcare Professional"
    return None


def extract_analysis_from_text(cv_text: str) -> dict:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ù…Ù† Ø§Ù„Ù†Øµ Ù…Ø¨Ø§Ø´Ø±Ø© (Ø¨Ø¯ÙˆÙ† API)"""
    text_lower = cv_text.lower()
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ù‡Ø§Ø±Ø§Øª
    all_skills = [
        'python', 'javascript', 'typescript', 'java', 'c++', 'c#', 'php', 'ruby', 'go', 'rust',
        'react', 'vue', 'angular', 'nodejs', 'express', 'django', 'flask', 'spring',
        'mongodb', 'postgresql', 'mysql', 'redis', 'docker', 'kubernetes',
        'aws', 'azure', 'gcp', 'git', 'linux', 'html', 'css', 'sql',
        'machine learning', 'tensorflow', 'pytorch', 'pandas', 'numpy', 'rest api'
    ]
    
    found_skills = [skill for skill in all_skills if skill in text_lower]
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø³Ù†ÙˆØ§Øª Ø§Ù„Ø®Ø¨Ø±Ø©
    experience_years = 0
    import re
    years_match = re.search(r'(\d+)\s*(?:years?|yrs?|Ø³Ù†Ø©|Ø³Ù†ÙˆØ§Øª)', text_lower)
    if years_match:
        experience_years = int(years_match.group(1))
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ©
    languages = []
    lang_keywords = {
        'Python': 'python',
        'JavaScript': 'javascript',
        'Java': 'java',
        'C++': 'c++',
        'C#': 'c#',
        'PHP': 'php',
        'Ruby': 'ruby',
        'Go': 'go',
        'TypeScript': 'typescript'
    }
    
    for lang_name, keyword in lang_keywords.items():
        if keyword in text_lower:
            languages.append(lang_name)

    # ØªØ­Ø¯ÙŠØ¯ Ø¯ÙˆØ± Ø¹Ø§Ù… (Ù…Ø«Ù„ Ø§Ù„Ø±Ø¹Ø§ÙŠØ© Ø§Ù„ØµØ­ÙŠØ©) Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…Ù‡Ø§Ø±Ø§Øª ØªÙ‚Ù†ÙŠØ© Ù…ÙˆØ¬ÙˆØ¯Ø©
    domain_role = detect_domain_role(text_lower)
    primary_role = domain_role or "Software Developer"
    
    return {
        "primary_role": primary_role,
        "skills": found_skills[:15],  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ 15 Ù…Ù‡Ø§Ø±Ø©
        "experience_years": experience_years,
        "languages": languages,
        "projects": [],
        "recommended_categories": []
    }


def analyze_cv_with_groq(cv_text: str) -> dict:
    """ØªØ­Ù„ÙŠÙ„ CV Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Groq API"""
    if not groq_client:
        return {"error": "Groq client not available"}
    
    prompt = f"""
Analyze this CV and provide detailed information about the candidate's profile:

CV Text:
{cv_text}

Please provide:
1. Primary job role/title that best fits this candidate
2. Key technical skills mentioned
3. Years of experience (estimate if not explicitly stated)
4. Main programming languages
5. Notable projects or achievements
6. Recommended job categories (from: Frontend Developer, Backend Developer, Full Stack Developer, Mobile Developer, DevOps Engineer, Data Scientist, Machine Learning Engineer, UI/UX Designer, Software Engineer, Quality Assurance Engineer)

Format your response as JSON with these fields:
{{
    "primary_role": "...",
    "skills": ["skill1", "skill2", ...],
    "experience_years": number,
    "languages": ["lang1", "lang2", ...],
    "projects": ["project1", "project2", ...],
    "recommended_categories": ["category1", "category2", ...]
}}
"""
    
    try:
        chat_completion = groq_client.chat.completions.create(
            messages=[
                {
                    "role": "user",
                    "content": prompt,
                }
            ],
            model="llama3-8b-8192",  # Ø£Ùˆ Ø£ÙŠ Ù…ÙˆØ¯ÙŠÙ„ Ù…ØªØ§Ø­
            temperature=0.3,
            max_tokens=1024,
        )
        
        response_text = chat_completion.choices[0].message.content
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ JSON Ù…Ù† Ø§Ù„Ø±Ø¯
        json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
        if json_match:
            analysis = json.loads(json_match.group())
            return analysis
        else:
            return {"raw_response": response_text}
            
    except Exception as e:
        return {"error": str(e)}


def classify_with_keywords(cv_text: str) -> dict:
    """ØªØµÙ†ÙŠÙ Ø¨Ø³ÙŠØ· Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… keyword matching"""
    text_lower = cv_text.lower()
    
    # ØªØ¹Ø±ÙŠÙ keywords Ù„ÙƒÙ„ ÙØ¦Ø©
    job_keywords = {
        "Frontend Developer": ['react', 'vue', 'angular', 'javascript', 'html', 'css', 'frontend', 'ui', 'typescript', 'next.js'],
        "Backend Developer": ['node', 'python', 'java', 'django', 'flask', 'spring', 'backend', 'api', 'express', 'fastapi'],
        "Full Stack Developer": ['full stack', 'fullstack', 'mern', 'mean', 'full-stack', 'lamp'],
        "Mobile Developer": ['android', 'ios', 'react native', 'flutter', 'swift', 'kotlin', 'mobile', 'app'],
        "DevOps Engineer": ['docker', 'kubernetes', 'aws', 'azure', 'devops', 'ci/cd', 'jenkins', 'terraform'],
        "Data Scientist": ['data science', 'machine learning', 'pandas', 'numpy', 'python', 'tensorflow', 'pytorch'],
        "Machine Learning Engineer": ['machine learning', 'deep learning', 'ai', 'neural', 'tensorflow', 'pytorch', 'keras'],
    }
    
    # Ø§Ø­Ø³Ø¨ score Ù„ÙƒÙ„ ÙØ¦Ø©
    scores = {}
    for job_title, keywords in job_keywords.items():
        score = sum(1 for keyword in keywords if keyword in text_lower)
        scores[job_title] = score
    
    # Ø§Ø­ØµÙ„ Ø¹Ù„Ù‰ Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø£Ø¹Ù„Ù‰
    best_job = max(scores, key=scores.get)
    best_score = scores[best_job]
    
    # Ø­ÙˆÙ„ score Ø¥Ù„Ù‰ confidence (normalized)
    max_possible_score = max(len(kw) for kw in job_keywords.values())
    confidence = min(best_score / max_possible_score * 100, 100) / 100
    confidence = max(confidence, 0.5)  # Ø­Ø¯ Ø£Ø¯Ù†Ù‰ 50% Ø¥Ø°Ø§ ÙˆØ¬Ø¯Ù†Ø§ Ø£ÙŠ keywords
    
    if best_score == 0:
        confidence = 0.0
    
    return {
        "predicted_job": best_job,
        "confidence": confidence,
        "method": "keyword_matching",
        "scores": scores
    }


@app.post("/classify", response_model=CVClassificationResponse)
async def classify_cv(request: CVClassificationRequest):
    """
    ØªØµÙ†ÙŠÙ CV Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ + AI analysis
    """
    try:
        cv_text = request.cv_text.strip()
        
        if not cv_text:
            print("âŒ CV text is empty")
            raise HTTPException(status_code=400, detail="CV text is required")
        
        print(f"ğŸ“„ CV Text Length: {len(cv_text)} characters")
        print(f"ğŸ“š First 200 chars: {cv_text[:200]}")
        
        # 1. Ø§Ø³ØªØ®Ø¯Ø§Ù… Keyword matching Ù„Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø³Ø±ÙŠØ¹
        print(f"ğŸ¯ Using Keyword-based Classification")
        keyword_result = classify_with_keywords(cv_text)
        print(f"ğŸ“Š Keyword Result: {keyword_result}")
        keyword_scores = keyword_result.get("scores", {})
        max_keyword_score = max(keyword_scores.values()) if keyword_scores else 0
        
        # 2. Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¯Ø§Ø¦Ù…Ø§Ù‹ (Groq Ø£Ùˆ Text Extraction)
        ai_analysis = None
        final_job_title = keyword_result.get("predicted_job", "Unknown")
        final_confidence = keyword_result.get("confidence", 0.0)
        decision_method = "keyword_matching"
        
        print(f"ğŸ’¼ Initial Job (Keyword): {final_job_title} ({final_confidence*100:.1f}%) | max_score={max_keyword_score}")
        
        # Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¯Ø§Ø¦Ù…Ø§Ù‹
        print("ğŸ¤– Analyzing CV...")
        if groq_client:
            print("   Using Groq AI for analysis...")
            ai_analysis = analyze_cv_with_groq(cv_text)
        else:
            print("   Using text extraction for analysis...")
            ai_analysis = extract_analysis_from_text(cv_text)
        
        print(f"ğŸ¤– Analysis Result: {ai_analysis}")
        
        if ai_analysis and "primary_role" in ai_analysis:
            ai_role = ai_analysis["primary_role"]
            print(f"ğŸ¤– AI Role: {ai_role}")
            
            # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø«Ù‚Ø© Ù…Ù†Ø®ÙØ¶Ø© Ø£Ùˆ Ù„Ù… Ù†Ø¬Ø¯ ÙƒÙ„Ù…Ø§Øª ØªÙ‚Ù†ÙŠØ©ØŒ Ø§Ø³ØªØ®Ø¯Ù… AI/domain role
            if final_confidence < 0.65 or max_keyword_score == 0:
                final_job_title = ai_role
                final_confidence = 0.85 if ai_role else 0.6
                decision_method = "ai_override_low_confidence"
                print(f"âœ… Override with AI/domain role: {final_job_title}")
            else:
                # Ø§Ù„Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© Ù…Ù† KeywordsØŒ Ø§Ø­ØªÙØ¸ Ø¨Ù‡Ø§
                decision_method = "keyword_matching_validated"
                print(f"âœ… Using Keywords (validated by AI): {final_job_title}")
        else:
            print("âš ï¸  Analysis failed")
        
        print(f"âœ… Final Result: {final_job_title} ({final_confidence*100:.1f}%)")
        
        # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ù„Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
        response_data = {
            "job_title": final_job_title,
            "confidence": final_confidence,
            "decision_method": decision_method,
            "ai_analysis": ai_analysis,
            "keras_prediction": keyword_result
        }
        
        return CVClassificationResponse(
            success=True,
            **response_data
        )
        
    except Exception as e:
        print(f"âŒ Error in classify_cv: {e}")
        import traceback
        traceback.print_exc()
        return CVClassificationResponse(
            success=False,
            job_title="Error",
            confidence=0.0,
            error=str(e)
        )


@app.get("/")
async def root():
    """ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    return {
        "service": "CV Classification Service",
        "status": "running",
        "keras_model": "loaded" if model else "not loaded",
        "groq_api": "available" if groq_client else "not available",
        "endpoints": {
            "classify": "/classify (POST)",
            "health": "/health (GET)"
        }
    }


@app.get("/health")
async def health():
    """ÙØ­Øµ Ø­Ø§Ù„Ø© Ø§Ù„Ø³ÙŠØ±ÙØ±"""
    return {
        "status": "healthy",
        "keras_model": model is not None,
        "groq_api": groq_client is not None
    }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=5002)
