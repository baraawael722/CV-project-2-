"""
Ù†Ù…ÙˆØ°Ø¬ Deep Learning Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø³ÙŠØ±Ø© Ø§Ù„Ø°Ø§ØªÙŠØ© Ù…Ø¹ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù
ÙŠØ³ØªØ®Ø¯Ù… BERT Ùˆ Sentence Transformers Ù„ØªØ­Ù‚ÙŠÙ‚ Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©
"""

import sys
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sentence_transformers import SentenceTransformer, util
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import pickle
import warnings
warnings.filterwarnings('ignore')


class CVJobDataset(Dataset):
    """Dataset Ù…Ø®ØµØµ Ù„Ù„Ø³ÙŠØ± Ø§Ù„Ø°Ø§ØªÙŠØ© ÙˆØ§Ù„ÙˆØ¸Ø§Ø¦Ù"""

    def __init__(self, cv_embeddings, job_embeddings, labels):
        self.cv_embeddings = cv_embeddings
        self.job_embeddings = job_embeddings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return {
            'cv_embedding': torch.FloatTensor(self.cv_embeddings[idx]),
            'job_embedding': torch.FloatTensor(self.job_embeddings[idx]),
            'label': torch.LongTensor([self.labels[idx]])
        }


class SiameseMatchingNetwork(nn.Module):
    """
    Ø´Ø¨ÙƒØ© Siamese Network Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø³ÙŠØ±Ø© Ø§Ù„Ø°Ø§ØªÙŠØ© Ù…Ø¹ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù
    ØªØ³ØªØ®Ø¯Ù… Attention Mechanism Ùˆ Residual Connections
    """

    def __init__(self, embedding_dim=384, hidden_dims=[512, 256, 128], dropout=0.3):
        super(SiameseMatchingNetwork, self).__init__()

        # CV Processing Branch
        self.cv_branch = nn.Sequential(
            nn.Linear(embedding_dim, hidden_dims[0]),
            nn.BatchNorm1d(hidden_dims[0]),
            nn.ReLU(),
            nn.Dropout(dropout),

            nn.Linear(hidden_dims[0], hidden_dims[1]),
            nn.BatchNorm1d(hidden_dims[1]),
            nn.ReLU(),
            nn.Dropout(dropout),

            nn.Linear(hidden_dims[1], hidden_dims[2]),
            nn.BatchNorm1d(hidden_dims[2]),
            nn.ReLU()
        )

        # Job Processing Branch
        self.job_branch = nn.Sequential(
            nn.Linear(embedding_dim, hidden_dims[0]),
            nn.BatchNorm1d(hidden_dims[0]),
            nn.ReLU(),
            nn.Dropout(dropout),

            nn.Linear(hidden_dims[0], hidden_dims[1]),
            nn.BatchNorm1d(hidden_dims[1]),
            nn.ReLU(),
            nn.Dropout(dropout),

            nn.Linear(hidden_dims[1], hidden_dims[2]),
            nn.BatchNorm1d(hidden_dims[2]),
            nn.ReLU()
        )

        # Attention Layer
        self.attention = nn.MultiheadAttention(
            embed_dim=hidden_dims[2],
            num_heads=4,
            dropout=dropout,
            batch_first=True
        )

        # Matching Network
        combined_dim = hidden_dims[2] * 2
        self.matching_network = nn.Sequential(
            nn.Linear(combined_dim, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(dropout),

            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(dropout),

            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(dropout / 2),

            nn.Linear(64, 1),
            nn.Sigmoid()
        )

    def forward(self, cv_embedding, job_embedding):
        # Process CV
        cv_features = self.cv_branch(cv_embedding)

        # Process Job
        job_features = self.job_branch(job_embedding)

        # Apply Attention (reshape for attention mechanism)
        cv_attn = cv_features.unsqueeze(1)
        job_attn = job_features.unsqueeze(1)

        # Cross attention between CV and Job
        cv_attended, _ = self.attention(cv_attn, job_attn, job_attn)
        cv_attended = cv_attended.squeeze(1)

        # Combine features
        combined = torch.cat([cv_attended, job_features], dim=1)

        # Calculate matching score
        similarity = self.matching_network(combined)

        return similarity


class CVJobMatcher:
    """
    Ù†Ø¸Ø§Ù… Ù…ØªÙƒØ§Ù…Ù„ Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø³ÙŠØ±Ø© Ø§Ù„Ø°Ø§ØªÙŠØ© Ù…Ø¹ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù
    """

    def __init__(self, model_name='all-MiniLM-L6-v2'):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        model_name: Ø§Ø³Ù… Ù†Ù…ÙˆØ°Ø¬ Sentence Transformer
        """
        print("ğŸš€ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ BERT...", file=sys.stderr, flush=True)
        self.device = torch.device(
            'cuda' if torch.cuda.is_available() else 'cpu')
        print(f"âœ… Ø§Ø³ØªØ®Ø¯Ø§Ù…: {self.device}", file=sys.stderr, flush=True)

        # ØªØ­Ù…ÙŠÙ„ Sentence Transformer
        self.embedder = SentenceTransformer(model_name)
        self.embedding_dim = self.embedder.get_sentence_embedding_dimension()

        # ØªÙ‡ÙŠØ¦Ø© Ø´Ø¨ÙƒØ© Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©
        self.matching_model = None
        self.label_encoder = LabelEncoder()

    def create_training_data(self, cvs_df, jobs_df, sample_size=10000):
        """
        Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…ØªÙˆØ§Ø²Ù†Ø©
        """
        print("\nğŸ“Š Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨...")

        training_data = []

        # Ø¥Ù†Ø´Ø§Ø¡ Ø£Ù…Ø«Ù„Ø© Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ© (CV ÙŠØªØ·Ø§Ø¨Ù‚ Ù…Ø¹ ÙØ¦ØªÙ‡)
        for idx, cv_row in cvs_df.iterrows():
            category = cv_row['Category']
            cv_text = cv_row['Resume']

            # Ø§Ø®ØªÙŠØ§Ø± ÙˆØ¸Ø§Ø¦Ù Ù…Ù† Ù†ÙØ³ Ø§Ù„ÙØ¦Ø©
            matching_jobs = jobs_df[jobs_df['Job Title'].str.contains(
                category, case=False, na=False)]

            if len(matching_jobs) > 0:
                # Ø§Ø®ØªÙŠØ§Ø± ÙˆØ¸ÙŠÙØ© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©
                job_row = matching_jobs.sample(1).iloc[0]
                training_data.append({
                    'cv': cv_text,
                    'job': f"{job_row['Job Title']} {job_row['job_description_clean']}",
                    'label': 1  # Ù…ØªØ·Ø§Ø¨Ù‚
                })

        # Ø¥Ù†Ø´Ø§Ø¡ Ø£Ù…Ø«Ù„Ø© Ø³Ù„Ø¨ÙŠØ© (CV Ù„Ø§ ÙŠØªØ·Ø§Ø¨Ù‚ Ù…Ø¹ Ø§Ù„ÙØ¦Ø©)
        for idx, cv_row in cvs_df.iterrows():
            category = cv_row['Category']
            cv_text = cv_row['Resume']

            # Ø§Ø®ØªÙŠØ§Ø± ÙˆØ¸Ø§Ø¦Ù Ù…Ù† ÙØ¦Ø© Ù…Ø®ØªÙ„ÙØ©
            non_matching_jobs = jobs_df[~jobs_df['Job Title'].str.contains(
                category, case=False, na=False)]

            if len(non_matching_jobs) > 0:
                job_row = non_matching_jobs.sample(1).iloc[0]
                training_data.append({
                    'cv': cv_text,
                    'job': f"{job_row['Job Title']} {job_row['job_description_clean']}",
                    'label': 0  # ØºÙŠØ± Ù…ØªØ·Ø§Ø¨Ù‚
                })

            if len(training_data) >= sample_size:
                break

        train_df = pd.DataFrame(training_data)
        print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ {len(train_df)} Ø¹ÙŠÙ†Ø© ØªØ¯Ø±ÙŠØ¨")
        print(f"   - Ø£Ù…Ø«Ù„Ø© Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©: {sum(train_df['label'] == 1)}")
        print(f"   - Ø£Ù…Ø«Ù„Ø© Ø³Ù„Ø¨ÙŠØ©: {sum(train_df['label'] == 0)}")

        return train_df

    def prepare_embeddings(self, texts, batch_size=32):
        """
        ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ embeddings
        """
        print(f"ğŸ”„ Ø¬Ø§Ø±ÙŠ ØªØ­ÙˆÙŠÙ„ {len(texts)} Ù†Øµ Ø¥Ù„Ù‰ embeddings...")
        embeddings = self.embedder.encode(
            texts,
            batch_size=batch_size,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        return embeddings

    def train(self, cvs_file, jobs_file, epochs=50, batch_size=32, learning_rate=0.001):
        """
        ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        """
        print("\n" + "="*60)
        print("ğŸ“ Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¯Ø±ÙŠØ¨")
        print("="*60)

        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        print("\nğŸ“‚ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
        cvs_df = pd.read_csv(cvs_file)
        jobs_df = pd.read_csv(jobs_file)

        print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(cvs_df)} Ø³ÙŠØ±Ø© Ø°Ø§ØªÙŠØ©")
        print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(jobs_df)} ÙˆØ¸ÙŠÙØ©")

        # Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        train_df = self.create_training_data(cvs_df, jobs_df)

        # ØªØ­Ø¶ÙŠØ± Embeddings
        print("\nğŸ”„ Ø¬Ø§Ø±ÙŠ ØªØ­Ø¶ÙŠØ± Embeddings...")
        cv_embeddings = self.prepare_embeddings(train_df['cv'].tolist())
        job_embeddings = self.prepare_embeddings(train_df['job'].tolist())

        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        X_cv_train, X_cv_val, X_job_train, X_job_val, y_train, y_val = train_test_split(
            cv_embeddings, job_embeddings, train_df['label'].values,
            test_size=0.2, random_state=42, stratify=train_df['label']
        )

        print(f"\nğŸ“Š ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:")
        print(f"   - Training: {len(y_train)} Ø¹ÙŠÙ†Ø©")
        print(f"   - Validation: {len(y_val)} Ø¹ÙŠÙ†Ø©")

        # Ø¥Ù†Ø´Ø§Ø¡ DataLoaders
        train_dataset = CVJobDataset(X_cv_train, X_job_train, y_train)
        val_dataset = CVJobDataset(X_cv_val, X_job_val, y_val)

        train_loader = DataLoader(
            train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size)

        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        print(f"\nğŸ—ï¸ Ø¬Ø§Ø±ÙŠ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ©...")
        self.matching_model = SiameseMatchingNetwork(
            embedding_dim=self.embedding_dim,
            hidden_dims=[512, 256, 128],
            dropout=0.3
        ).to(self.device)

        # Loss Ùˆ Optimizer
        criterion = nn.BCELoss()
        optimizer = torch.optim.AdamW(
            self.matching_model.parameters(),
            lr=learning_rate,
            weight_decay=0.01
        )

        # Learning Rate Scheduler
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='max', factor=0.5, patience=5
        )

        # Early Stopping
        best_val_acc = 0
        patience = 10
        patience_counter = 0

        # Training Loop
        print("\n" + "="*60)
        print("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨...")
        print("="*60)

        for epoch in range(epochs):
            # Training
            self.matching_model.train()
            train_loss = 0
            train_correct = 0
            train_total = 0

            for batch in train_loader:
                cv_emb = batch['cv_embedding'].to(self.device)
                job_emb = batch['job_embedding'].to(self.device)
                labels = batch['label'].float().to(self.device)

                optimizer.zero_grad()
                outputs = self.matching_model(cv_emb, job_emb)
                loss = criterion(outputs, labels)
                loss.backward()

                # Gradient Clipping
                torch.nn.utils.clip_grad_norm_(
                    self.matching_model.parameters(), max_norm=1.0)

                optimizer.step()

                train_loss += loss.item()
                predictions = (outputs > 0.5).float()
                train_correct += (predictions == labels).sum().item()
                train_total += labels.size(0)

            train_acc = 100 * train_correct / train_total
            avg_train_loss = train_loss / len(train_loader)

            # Validation
            self.matching_model.eval()
            val_loss = 0
            val_correct = 0
            val_total = 0

            with torch.no_grad():
                for batch in val_loader:
                    cv_emb = batch['cv_embedding'].to(self.device)
                    job_emb = batch['job_embedding'].to(self.device)
                    labels = batch['label'].float().to(self.device)

                    outputs = self.matching_model(cv_emb, job_emb)
                    loss = criterion(outputs, labels)

                    val_loss += loss.item()
                    predictions = (outputs > 0.5).float()
                    val_correct += (predictions == labels).sum().item()
                    val_total += labels.size(0)

            val_acc = 100 * val_correct / val_total
            avg_val_loss = val_loss / len(val_loader)

            # Print Progress
            print(f"Epoch [{epoch+1}/{epochs}]")
            print(
                f"  Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.2f}%")
            print(f"  Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.2f}%")
            print("-" * 60)

            # Learning Rate Scheduling
            scheduler.step(val_acc)

            # Early Stopping & Model Saving
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                patience_counter = 0
                # Ø­ÙØ¸ Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬
                torch.save(self.matching_model.state_dict(),
                           'best_matching_model.pth')
                print(
                    f"âœ… ØªÙ… Ø­ÙØ¸ Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬! Validation Accuracy: {val_acc:.2f}%\n")
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print(
                        f"\nâš ï¸ Early Stopping! Ù„Ù… ÙŠØªØ­Ø³Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù†Ø° {patience} epochs")
                    break

        # ØªØ­Ù…ÙŠÙ„ Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬
        self.matching_model.load_state_dict(
            torch.load('best_matching_model.pth'))

        print("\n" + "="*60)
        print(f"âœ… Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØªØ¯Ø±ÙŠØ¨!")
        print(f"ğŸ† Ø£ÙØ¶Ù„ Ø¯Ù‚Ø©: {best_val_acc:.2f}%")
        print("="*60)

        return best_val_acc

    def find_top_matches(self, cv_text, job_descriptions, top_k=10, use_hybrid=True):
        """
        Ø¥ÙŠØ¬Ø§Ø¯ Ø£ÙØ¶Ù„ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ù„Ù„Ø³ÙŠØ±Ø© Ø§Ù„Ø°Ø§ØªÙŠØ©
        use_hybrid: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù‡Ø¬ Ù‡Ø¬ÙŠÙ† ÙŠØ¬Ù…Ø¹ Ø¨ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ ÙˆØ§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±
        """
        # ØªØ­ÙˆÙŠÙ„ CV Ø¥Ù„Ù‰ embedding
        cv_embedding = self.embedder.encode([cv_text], convert_to_numpy=True)

        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø¥Ù„Ù‰ embeddings
        job_embeddings = self.embedder.encode(
            job_descriptions, convert_to_numpy=True)

        # Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø§Øª Ø§Ù„ØªØ·Ø§Ø¨Ù‚
        matches = []

        if self.matching_model is not None and not use_hybrid:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ ÙÙ‚Ø·
            self.matching_model.eval()
            with torch.no_grad():
                cv_tensor = torch.FloatTensor(cv_embedding).to(self.device)

                for idx, job_emb in enumerate(job_embeddings):
                    job_tensor = torch.FloatTensor(
                        job_emb).unsqueeze(0).to(self.device)
                    score = self.matching_model(cv_tensor, job_tensor)
                    matches.append({
                        'job_index': idx,
                        'similarity_score': score.item() * 100
                    })
        else:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ Ø§Ù„Ù…Ø¨Ø§Ø´Ø± (Ø£ÙƒØ«Ø± Ø¯Ù‚Ø© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©)
            # Ø­Ø³Ø§Ø¨ cosine similarity Ù…Ø¨Ø§Ø´Ø±Ø© Ù…Ù† BERT embeddings
            for idx, job_emb in enumerate(job_embeddings):
                # Cosine similarity
                cos_sim = util.cos_sim(cv_embedding[0], job_emb).item()

                # ØªØ­ÙˆÙŠÙ„ Ù…Ù† [-1, 1] Ø¥Ù„Ù‰ [0, 100]
                similarity_score = (cos_sim + 1) * 50

                # Ø¥Ø¶Ø§ÙØ© keyword matching boost
                keyword_boost = self._calculate_keyword_match(
                    cv_text, job_descriptions[idx])

                # Ø§Ù„Ø¯Ø±Ø¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©: 70% semantic + 30% keyword matching
                final_score = (similarity_score * 0.7) + (keyword_boost * 0.3)

                matches.append({
                    'job_index': idx,
                    'similarity_score': final_score
                })

        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        matches = sorted(
            matches, key=lambda x: x['similarity_score'], reverse=True)

        return matches[:top_k]

    def _calculate_keyword_match(self, cv_text, job_text):
        """
        Ø­Ø³Ø§Ø¨ Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ·Ø§Ø¨Ù‚ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„ØªÙ‚Ù†ÙŠØ©
        """
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ lowercase
        cv_lower = cv_text.lower()
        job_lower = job_text.lower()

        # Ù‚Ø§Ø¦Ù…Ø© Ø´Ø§Ù…Ù„Ø© Ø¨Ø§Ù„Ù…Ù‡Ø§Ø±Ø§Øª Ø§Ù„ØªÙ‚Ù†ÙŠØ© ÙˆØ§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
        tech_keywords = [
            # Backend & Languages
            'node.js', 'nodejs', 'express', 'express.js',
            'python', 'java', 'javascript', 'typescript', 'php', 'c#', 'c++',
            'ruby', 'go', 'golang', 'rust', 'scala', 'kotlin',

            # Databases
            'mongodb', 'mysql', 'postgresql', 'redis', 'sql', 'nosql',
            'database', 'oracle', 'cassandra', 'dynamodb',

            # Frontend
            'react', 'vue', 'angular', 'next.js', 'nextjs',
            'html', 'css', 'javascript', 'jquery', 'bootstrap',

            # DevOps & Tools
            'docker', 'kubernetes', 'jenkins', 'git', 'github', 'gitlab',
            'ci/cd', 'aws', 'azure', 'gcp', 'nginx', 'apache',
            'linux', 'unix', 'bash', 'shell',

            # API & Architecture
            'rest', 'restful', 'api', 'graphql', 'microservices',
            'websocket', 'grpc', 'soap',

            # Security & Auth
            'jwt', 'oauth', 'authentication', 'authorization',
            'security', 'encryption', 'ssl', 'tls',

            # AI & Data Science
            'machine learning', 'deep learning', 'tensorflow', 'pytorch',
            'scikit-learn', 'pandas', 'numpy', 'computer vision',
            'opencv', 'nlp', 'ai', 'artificial intelligence',

            # Mobile
            'react native', 'flutter', 'android', 'ios', 'swift',
            'kotlin', 'mobile app',

            # Testing & Quality
            'testing', 'unit test', 'selenium', 'jest', 'pytest',
            'qa', 'quality assurance', 'agile', 'scrum',

            # Data & Analytics
            'data analysis', 'power bi', 'tableau', 'excel',
            'analytics', 'big data', 'hadoop', 'spark',

            # Design & Marketing
            'photoshop', 'illustrator', 'figma', 'ui/ux',
            'seo', 'marketing', 'google ads',

            # Network & Systems
            'network', 'cisco', 'firewall', 'vpn', 'routing',
            'cybersecurity', 'penetration testing', 'siem',

            # Business & Management
            'project management', 'hr', 'accounting', 'quickbooks',
            'communication', 'leadership'
        ]

        # Ø­Ø³Ø§Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ù…Ø´ØªØ±ÙƒØ©
        matched_keywords = 0
        total_job_keywords = 0

        for keyword in tech_keywords:
            if keyword in job_lower:
                total_job_keywords += 1
                if keyword in cv_lower:
                    matched_keywords += 1

        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ø¦ÙˆÙŠØ©
        if total_job_keywords > 0:
            match_percentage = (matched_keywords / total_job_keywords) * 100
        else:
            match_percentage = 0

        return match_percentage

    def save_model(self, path='cv_job_matcher.pkl'):
        """
        Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        """
        model_data = {
            'matching_model_state': self.matching_model.state_dict(),
            'embedding_dim': self.embedding_dim,
            'embedder_name': self.embedder._model_config.get('_name_or_path', 'all-MiniLM-L6-v2')
        }

        with open(path, 'wb') as f:
            pickle.dump(model_data, f)

        print(f"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ: {path}")

    def load_model(self, path='cv_job_matcher.pkl'):
        """
        ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        """
        with open(path, 'rb') as f:
            model_data = pickle.load(f)

        self.embedding_dim = model_data['embedding_dim']
        self.matching_model = SiameseMatchingNetwork(
            embedding_dim=self.embedding_dim).to(self.device)
        self.matching_model.load_state_dict(model_data['matching_model_state'])
        self.matching_model.eval()

        print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù†: {path}", file=sys.stderr, flush=True)


def main():
    """
    Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ø§Ø®ØªØ¨Ø§Ø±
    """
    # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    matcher = CVJobMatcher(model_name='all-MiniLM-L6-v2')

    # Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    best_accuracy = matcher.train(
        cvs_file='dataa.csv',
        jobs_file='jobs_clean.csv',
        epochs=50,
        batch_size=32,
        learning_rate=0.001
    )

    # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    matcher.save_model('cv_job_matcher_final.pkl')

    print(f"\nğŸ‰ ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡! Ø£ÙØ¶Ù„ Ø¯Ù‚Ø©: {best_accuracy:.2f}%")


if __name__ == "__main__":
    main()
